<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Emotion AI Test</title>
  <style>
    body { background:#111; color:#fff; font-family:monospace; }
    video, canvas { border:1px solid #444; border-radius:8px; }
    #diag { background:#000; padding:8px; margin-top:10px; height:200px; overflow:auto; white-space:pre-wrap; }
  </style>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.18.0"></script>
  <script src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2"></script>
</head>
<body>
  <h2>Emotion AI – Webcam Test</h2>
  <video id="video" width="640" height="480" autoplay muted playsinline></video>
  <canvas id="overlay" width="640" height="480"></canvas>
  <div id="diag">DIAG:\n</div>

<script>
const diagEl = document.getElementById("diag");
function log(msg){ console.log(msg); diagEl.textContent += msg + "\n"; }

const video = document.getElementById("video");
const canvas = document.getElementById("overlay");
const ctx = canvas.getContext("2d");
const MODEL_URL = "https://geraldsathiyasiva.github.io/Emotion-Ai-/models";

async function startCam(){
  try {
    const stream = await navigator.mediaDevices.getUserMedia({video:true,audio:false});
    video.srcObject = stream;
    await video.play();
    log("Camera started: " + video.videoWidth + "x" + video.videoHeight);
  } catch(e){ log("Camera error: " + e.message); }
}

async function loadModels(){
  log("Loading models from " + MODEL_URL);
  await faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL);
  await faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL);
  await faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL);
  log("Models loaded ✔");
}

async function runTest(){
  log("Running 5s test...");
  let t0 = Date.now();
  let interval = setInterval(async ()=>{
    const det = await faceapi.detectAllFaces(video,new faceapi.TinyFaceDetectorOptions()).withFaceLandmarks().withFaceExpressions();
    ctx.clearRect(0,0,canvas.width,canvas.height);
    if(det.length>0){
      faceapi.draw.drawDetections(canvas,det);
      faceapi.draw.drawFaceLandmarks(canvas,det);
      log("Face detected. Emotions: " + JSON.stringify(det[0].expressions));
    } else {
      log("No face detected.");
    }
    if(Date.now()-t0 > 5000){ clearInterval(interval); log("Test done."); }
  },500);
}

(async function init(){
  log("BOOT: inline logger alive");
  await startCam();
  await loadModels();
  await runTest();
})();
</script>
</body>
</html>
