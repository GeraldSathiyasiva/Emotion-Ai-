<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Emotion Recognition Test + Ad Analysis</title>
<style>
  body { background:#0f1115; color:#e5e7eb; font-family:system-ui,Arial; margin:16px }
  video,canvas { border:1px solid #2a2f3d; border-radius:10px; background:#000 }
  #diag { white-space:pre-wrap; font:12px ui-monospace; background:#0b1020; border:1px solid #222; border-radius:10px; padding:10px; height:220px; overflow:auto; margin-top:10px }
  button { padding:8px 12px; border:0; border-radius:8px; background:#2a2f3d; color:#fff; cursor:pointer; margin-right:6px; }
</style>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.18.0"></script>
<script src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2"></script>
</head>
<body>
<h2>Emotion Recognition</h2>
<video id="cam" width="640" height="480" autoplay muted playsinline></video>
<canvas id="overlay" width="640" height="480"></canvas>
<div>
  <button id="btnTest" disabled>Run 5s Face/Emotion Test</button>
  <button id="btnAnalyse" disabled>Play & Analyse Ad</button>
</div>
<div id="diag">DIAG:\n</div>

<script>
const MODEL_URL = "https://geraldsathiyasiva.github.io/Emotion-Ai-/models";
const cam = document.getElementById('cam');
const overlay = document.getElementById('overlay');
const ctx = overlay.getContext('2d');
const diagEl = document.getElementById('diag');
const btnTest = document.getElementById('btnTest');
const btnAnalyse = document.getElementById('btnAnalyse');

function diag(msg) {
  console.log(msg);
  diagEl.textContent += msg + '\n';
  diagEl.scrollTop = diagEl.scrollHeight;
}

async function startWebcam() {
  try {
    const stream = await navigator.mediaDevices.getUserMedia({ video: {} });
    cam.srcObject = stream;
    await cam.play();
    overlay.width = cam.videoWidth;
    overlay.height = cam.videoHeight;
    diag('Webcam started');
  } catch (err) {
    diag('Webcam error: ' + err.message);
  }
}

async function loadModels() {
  diag('Loading models from GitHub...');
  await faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL);
  await faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL);
  await faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL);
  diag('Models loaded');
}

async function detectOnce() {
  const detections = await faceapi.detectAllFaces(cam, new faceapi.TinyFaceDetectorOptions())
    .withFaceLandmarks()
    .withFaceExpressions();
  ctx.clearRect(0, 0, overlay.width, overlay.height);
  ctx.drawImage(cam, 0, 0, overlay.width, overlay.height);
  if (detections.length > 0) {
    faceapi.draw.drawDetections(overlay, detections);
    faceapi.draw.drawFaceLandmarks(overlay, detections);
    const expr = detections[0].expressions;
    diag('Face OK — Emotions: ' + JSON.stringify(expr));
  } else {
    diag('No face detected');
  }
}

btnTest.addEventListener('click', async () => {
  diag('Running 5-second face/emotion test...');
  const start = Date.now();
  const interval = setInterval(detectOnce, 500);
  setTimeout(() => {
    clearInterval(interval);
    diag('5-second test complete');
    btnAnalyse.disabled = false;
  }, 5000);
});

btnAnalyse.addEventListener('click', () => {
  diag('Ad analysis placeholder — integrate your existing analysis code here.');
  // This is where your ad analysis logic will run after emotions are confirmed working
});

(async function init() {
  await startWebcam();
  await loadModels();
  btnTest.disabled = false;
})();
</script>
</body>
</html>
