<!DOCTYPE html>
<html>
<head>
  <title>Emotion Ad Test</title>
  <meta charset="utf-8">
  <script defer src="https://cdn.jsdelivr.net/npm/face-api.js"></script>
  <style>
    body { font-family: sans-serif; margin: 20px; }
    #diag { font-size: 14px; white-space: pre; background: #eee; padding: 10px; }
  </style>
</head>
<body>

<h1>Emotion-Only Ad Test</h1>

<label>Ad URL:</label>
<input id="adUrl" value="ads/0813.mp4"><br><br>
<video id="adVideo" width="480" controls></video>
<video id="cam" autoplay muted playsinline width="320" height="240"></video>
<canvas id="overlay" width="320" height="240"></canvas>

<p><button id="loadModels">Load Models</button> 
<button id="playAnalyze">Play & Analyze</button></p>

<div id="diag">DIAG: Waiting…</div>

<h2>Results</h2>
<p>Purchase Intention: <span id="pi">0</span>%</p>
<p>Brand Perception: <span id="bp">0</span>%</p>
<p>Awareness: <span id="awr">0</span>%</p>

<script>
let detections = [];
let faceFrames = 0;

async function loadModels() {
  diag("Loading models...");
  await faceapi.nets.tinyFaceDetector.loadFromUri('./models');
  await faceapi.nets.faceLandmark68Net.loadFromUri('./models');
  await faceapi.nets.faceExpressionNet.loadFromUri('./models');
  diag("MODELS: OK ✔ tinyFace + landmarks68 + expressions");
}

function diag(msg) {
  document.getElementById("diag").textContent = msg;
}

async function startCam() {
  const stream = await navigator.mediaDevices.getUserMedia({ video: true });
  const cam = document.getElementById('cam');
  cam.srcObject = stream;
}

async function analyzeFrame(video) {
  const det = await faceapi.detectSingleFace(video, new faceapi.TinyFaceDetectorOptions({ inputSize: 224, scoreThreshold: 0.1 }))
    .withFaceLandmarks()
    .withFaceExpressions();
  
  if (det) {
    faceFrames++;
    const e = det.expressions;
    detections.push({
      happy: e.happy || 0,
      sad: e.sad || 0,
      angry: e.angry || 0,
      surprised: e.surprised || 0,
      disgusted: e.disgusted || 0,
      fearful: e.fearful || 0
    });

    diag(`faces so far: ${faceFrames} | happy=${e.happy.toFixed(2)} sad=${e.sad.toFixed(2)} angry=${e.angry.toFixed(2)} surprised=${e.surprised.toFixed(2)}`);
  }
}

function calculateKPIs() {
  if (detections.length === 0) return;

  const posFrames = detections.filter(d => d.happy > 0.3 || d.surprised > 0.3).length;
  const negFrames = detections.filter(d => d.sad > 0.3 || d.angry > 0.3 || d.disgusted > 0.3).length;
  const awareFrames = detections.filter(d => 
    d.happy > 0.15 || d.sad > 0.15 || d.angry > 0.15 || d.surprised > 0.15
  ).length;

  const total = detections.length;
  const PI = (posFrames / total * 100).toFixed(1);
  const BP = ((posFrames - negFrames) / total * 100 + 50).toFixed(1); // scaled
  const AWR = (awareFrames / total * 100).toFixed(1);

  document.getElementById("pi").textContent = PI;
  document.getElementById("bp").textContent = BP;
  document.getElementById("awr").textContent = AWR;
}

document.getElementById("loadModels").addEventListener("click", loadModels);

document.getElementById("playAnalyze").addEventListener("click", async () => {
  detections = [];
  const adVideo = document.getElementById('adVideo');
  adVideo.src = document.getElementById('adUrl').value;
  await adVideo.play();

  const cam = document.getElementById('cam');
  const overlay = document.getElementById('overlay');
  const ctx = overlay.getContext('2d');

  const interval = setInterval(async () => {
    await analyzeFrame(cam);
    ctx.clearRect(0, 0, overlay.width, overlay.height);
    const det = await faceapi.detectSingleFace(cam, new faceapi.TinyFaceDetectorOptions({ inputSize: 224, scoreThreshold: 0.1 }))
      .withFaceLandmarks()
      .withFaceExpressions();
    if (det) {
      faceapi.draw.drawDetections(overlay, [det]);
      faceapi.draw.drawFaceLandmarks(overlay, [det]);
    }
  }, 200);

  adVideo.onended = () => {
    clearInterval(interval);
    calculateKPIs();
    diag("Analysis complete.");
  };
});

startCam();
</script>

</body>
</html>
