<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Emotion Detection Demo</title>

  <!-- face-api.js library -->
  <script src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>

  <style>
    :root { color-scheme: dark; }
    body { margin:0; display:flex; flex-direction:column; align-items:center; justify-content:center; min-height:100vh; background:#111; color:#fff; font-family:system-ui, -apple-system, Arial; }
    #container { position:relative; width:100%; max-width:420px; }
    video { width:100%; border-radius:12px; display:block; background:#000; }
    canvas { position:absolute; top:0; left:0; pointer-events:none; }
    #status { margin-top:12px; font-size:15px; opacity:.9; }
    button { margin-top:12px; padding:10px 14px; border:0; border-radius:10px; background:#2a2a2a; color:#fff; }
    .error { color:#ff8a8a; }
  </style>
</head>
<body>
  <div id="container">
    <video id="video" autoplay muted playsinline></video>
    <!-- canvas will be injected -->
  </div>
  <div id="status">Loading models…</div>
  <button id="retry" style="display:none;">Retry</button>

  <script>
    const statusEl = document.getElementById('status');
    const retryBtn = document.getElementById('retry');
    const video = document.getElementById('video');
    const container = document.getElementById('container');
    const MODEL_URL = './models'; // <-- uses your repo’s models folder

    // Helper to show errors on screen
    function showError(msg) {
      statusEl.classList.add('error');
      statusEl.textContent = msg;
      retryBtn.style.display = 'inline-block';
    }

    window.addEventListener('error', e => showError('Page error: ' + e.message));
    window.addEventListener('unhandledrejection', e => showError('Promise error: ' + (e.reason && e.reason.message || e.reason)));

    async function loadModels() {
      try {
        statusEl.textContent = 'Loading models…';
        await faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL);
        await faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL);
        statusEl.textContent = 'Starting camera…';
      } catch (err) {
        showError('Model load failed. Check /models files. ' + err);
        throw err;
      }
    }

    async function startCamera() {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: 'user' }, audio: false });
        video.srcObject = stream;
        statusEl.textContent = 'Camera started.';
      } catch (err) {
        showError('Camera error: ' + err.message);
        throw err;
      }
    }

    function startDetection() {
      const canvas = faceapi.createCanvasFromMedia(video);
      canvas.style.top = '0';
      canvas.style.left = '0';
      container.appendChild(canvas);

      const setSize = () => {
        const w = video.videoWidth || video.clientWidth;
        const h = video.videoHeight || video.clientHeight;
        canvas.width = w; canvas.height = h;
        faceapi.matchDimensions(canvas, { width: w, height: h });
      };
      video.addEventListener('loadedmetadata', setSize);
      video.addEventListener('resize', setSize);

      const opts = new faceapi.TinyFaceDetectorOptions({ inputSize: 224, scoreThreshold: 0.5 });

      const loop = async () => {
        try {
          const w = video.videoWidth, h = video.videoHeight;
          if (!w || !h) return requestAnimationFrame(loop);

          const det = await faceapi.detectSingleFace(video, opts).withFaceExpressions();
          const ctx = canvas.getContext('2d');
          ctx.clearRect(0, 0, canvas.width, canvas.height);

          if (det && det.expressions) {
            const resized = faceapi.resizeResults(det, { width: canvas.width, height: canvas.height });
            faceapi.draw.drawDetections(canvas, resized);
            faceapi.draw.drawFaceExpressions(canvas, resized);

            const exps = det.expressions;
            cons
