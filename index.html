<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Emotion + Subtle Cues (fused labels)</title>

  <!-- face-api.js -->
  <script src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>

  <style>
    :root { color-scheme: dark; }
    body { margin:0; min-height:100vh; display:flex; flex-direction:column; align-items:center; justify-content:center; background:#111; color:#fff; font-family:system-ui,-apple-system,Arial; }
    #container { position:relative; width:100%; max-width:420px; }
    video { width:100%; border-radius:12px; background:#000; display:block; }
    canvas { position:absolute; top:0; left:0; pointer-events:none; }
    #hud { width:100%; max-width:420px; display:flex; align-items:center; justify-content:space-between; gap:10px; margin-top:10px; }
    #status { opacity:.95; }
    #fps { opacity:.7; }
    .chip {
      position:absolute; left:50%; transform:translateX(-50%);
      background:rgba(255,255,255,.12); padding:6px 12px; border-radius:999px;
      font-weight:600; backdrop-filter: blur(6px); box-shadow:0 6px 18px rgba(0,0,0,.2);
    }
    #chipMain { top:10px; }
    #chipSubtle { top:44px; display:none; background:rgba(88,170,255,.18); }
    .row { margin-top:10px; display:flex; gap:8px; align-items:center; }
    button { padding:8px 12px; border:0; border-radius:10px; background:#2a2a2a; color:#fff; }
  </style>
</head>
<body>
  <div id="container">
    <video id="video" autoplay muted playsinline></video>
    <!-- overlay chips -->
    <div id="chipMain"   class="chip">ðŸ™‚ Neutral</div>
    <div id="chipSubtle" class="chip">Subtle: â€”</div>
  </div>

  <div id="hud">
    <div id="status">Loading modelsâ€¦</div>
    <div id="fps"></div>
  </div>

  <div class="row">
    <button id="recal">Re-calibrate (neutral)</button>
    <button id="perf">Performance: High</button>
  </div>

  <script>
    // Local models already in your repo (tinyFace + expressions)
    const MODEL_URL = './models';
    // Hosted landmarks (no download needed)
    const MODEL_URL_LM = 'https://raw.githubusercontent.com/justadudewhohacks/face-api.js-models/master/face_landmark_68';

    const video = document.getElementById('video');
    const container = document.getElementById('container');
    const statusEl = document.getElementById('status');
    const fpsEl = document.getElementById('fps');
    const chipMain = document.getElementById('chipMain');
    const chipSubtle = document.getElementById('chipSubtle');
    const btnRecal = document.getElementById('recal');
    const btnPerf = document.getElementById('perf');

    let highPerf = true; // larger detector input + higher FPS hint
    const EMOJI = { neutral:'ðŸ˜', happy:'ðŸ˜ƒ', sad:'ðŸ˜¢', angry:'ðŸ˜ ', fearful:'ðŸ˜¨', disgusted:'ðŸ¤¢', surprised:'ðŸ˜®' };

    function showErr(m){ statusEl.textContent = m; statusEl.style.color = '#ff8a8a'; }

    async function loadModels(){
      statusEl.textContent = 'Loading modelsâ€¦';
      await faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL);
      await faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL);
      await faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL_LM); // hosted
      statusEl.textContent = 'Models loaded. Starting cameraâ€¦';
    }

    async function startCamera(){
      const stream = await navigator.mediaDevices.getUserMedia({
        video: {
          facingMode:'user',
          width:{ ideal:1280 }, height:{ ideal:720 },
          frameRate: highPerf ? { ideal:60, max:120 } : { ideal:30, max:30 }
        },
        audio:false
      });
      video.srcObject = stream;
      await new Promise(res => { if (video.readyState >= 2) res(); else video.onloadedmetadata = () => res(); });
      try { await video.play(); } catch(_) {}
      statusEl.textContent = 'Camera started.';
    }

    function prepareCanvas(){
      const canvas = document.createElement('canvas');
      const sync=()=>{ const w=video.videoWidth||640, h=video.videoHeight||480; canvas.width=w; canvas.height=h; };
      sync(); container.appendChild(canvas);
      video.addEventListener('loadeddata', sync); video.addEventListener('resize', sync);
      return canvas;
    }

    // ---------- Landmark utilities ----------
    const dist = (a,b)=>Math.hypot(a.x-b.x, a.y-b.y);
    const vdist = (a,b)=>Math.abs(a.y - b.y);
    const avgY = (pts, idxs) => idxs.reduce((s,i)=>s+pts[i].y,0)/idxs.length;

    const LEFT_EYE   = [36,37,38,39,40,41];
    const RIGHT_EYE  = [42,43,44,45,46,47];
    const LEFT_BROW  = [17,18,19,20,21];
    const RIGHT_BROW = [22,23,24,25,26];
    const NOSE_L = 31, NOSE_R = 35, NOSE_TOP = 27, CHIN = 8;
    const M_L = 48, M_R = 54, M_TOP = 51, M_BOT = 57;
    function eyeOuterCrinkle(p){
      const l = (vdist(p[38], p[40]) + vdist(p[39], p[41])) / 2;
      const r = (vdist(p[44], p[46]) + vdist(p[45], p[47])) / 2;
      return (l + r) / 2;
    }

    // Baseline learned during calibration
    const base = { eyeAperture:0, browEyeGap:0, mouthSmile:0, mouthFrown:0, lipGap:0, outerEyeGap:0 };
    let calibrated = false;

    function calcFeatures(landmarks){
      const p = landmarks.positions;
      const faceH = dist(p[NOSE_TOP], p[CHIN]) || 1;

      // Eye aperture (avg vertical opening)
      const eyeApertureL = (p[38].y - p[40].y);
      const eyeApertureR = (p[44].y - p[46].y);
      const eyeAperture  = ((eyeApertureL + eyeApertureR)/2) / faceH;

      // Browâ€“eye gap (avg brow to eye-top distance)
      const browY_L = avgY(p, LEFT_BROW), browY_R = avgY(p, RIGHT_BROW);
      const eyeTopY_L = (p[37].y + p[38].y)/2, eyeTopY_R = (p[43].y + p[44].y)/2;
      const browEyeGap = (((eyeTopY_L - browY_L) + (eyeTopY_R - browY_R))/2) / faceH;

      // Smile proxy: mouth width normalized by nose width
      const mouthW = dist(p[M_L], p[M_R]);
      const noseW  = dist(p[NOSE_L], p[NOSE_R]) || 1;
      const mouthSmile = mouthW / noseW;

      // Frown proxy: corners below mouth midline
      const mouthMidY = (p[M_TOP].y + p[M_BOT].y)/2;
      const cornerDrop = ((p[M_L].y - mouthMidY) + (p[M_R].y - mouthMidY))/2;
      const mouthFrown = cornerDrop / faceH;

      // Lip press: upperâ€“lower inner lip gap shrinks
      const lipGap = vdist(p[62], p[66]) / faceH;

      // Eye crinkle: smaller outer-lid opening near corners
      const outerEyeGap = eyeOuterCrinkle(p) / faceH;

      return { eyeAperture, browEyeGap, mouthSmile, mouthFrown, lipGap, outerEyeGap };
    }

    async function calibrate(detector, ms=2000){
      statusEl.textContent = 'Calibratingâ€¦ relax face for 2s';
      const t0 = performance.now(); const samples = [];
      while (performance.now() - t0 < ms){
        const det = await detector();
        if (det?.landmarks){ samples.push(calcFeatures(det.landmarks)); }
        await new Promise(r=>requestAnimationFrame(r));
      }
      if (!samples.length) return false;
      const mean = k => samples.reduce((s,o)=>s+o[k],0)/samples.length;
      base.eyeAperture = mean('eyeAperture');
      base.browEyeGap  = mean('browEyeGap');
      base.mouthSmile  = mean('mouthSmile');
      base.mouthFrown  = mean('mouthFrown');
      base.lipGap      = mean('lipGap');
      base.outerEyeGap = mean('outerEyeGap');
      calibrated = true;
      statusEl.textContent = 'Calibrated. Trackingâ€¦';
      return true;
    }

    // Fusion: expressions + subtle feature deltas â†’ final label
    function labelEmotion(exps, feat){
      const d = {
        browUp:   (feat.browEyeGap - base.browEyeGap),
        browDown: (base.browEyeGap - feat.browEyeGap),
        eyesOpen: (feat.eyeAperture - base.eyeAperture),
        smile:    (feat.mouthSmile - base.mouthSmile),
        frown:    (feat.mouthFrown - base.mouthFrown),
        lipPress: (base.lipGap - feat.lipGap),
        crinkle:  (base.outerEyeGap - feat.outerEyeGap)
      };

      const s = {
        happy:     (exps.happy||0)     * 1.0,
        sad:       (exps.sad||0)       * 1.0,
        angry:     (exps.angry||0)     * 1.0,
        fearful:   (exps.fearful||0)   * 1.0,
        disgusted: (exps.disgusted||0) * 1.0,
        surprised: (exps.surprised||0) * 1.0,
        neutral:   (exps.neutral||0)   * 1.0
      };

      // Add landmark evidence (tune weights to taste)
      s.surprised +=  2.0 * Math.max(0, d.browUp)   + 1.6 * Math.max(0, d.eyesOpen);
      s.happy     +=  2.2 * Math.max(0, d.smile)    + 1.4 * Math.max(0, d.crinkle);
      s.angry     +=  1.8 * Math.max(0, d.browDown) + 1.6 * Math.max(0, d.lipPress);
      s.sad       +=  1.6 * Math.max(0, d.frown);
      s.fearful   +=  1.2 * Math.max(0, d.eyesOpen) + 0.8 * Math.max(0, d.browUp);
      s.neutral   +=  0.5 * (d.smile < 0 && d.frown < 0 && d.eyesOpen < 0 ? 1 : 0);

      let best='neutral', bestVal=-1;
      for (const [k,v] of Object.entries(s)) if (v > bestVal){ best=k; bestVal=v; }
      return best;
    }

    function subtleCueText(feat){
      const cues = [];
      if (feat.browEyeGap - base.browEyeGap > 0.015) cues.push('Brow â†‘');
      if (base.browEyeGap - feat.browEyeGap > 0.015) cues.push('Brow â†“');
      if (feat.eyeAperture - base.eyeAperture > 0.012) cues.push('Eyes â†‘');
      if (feat.mouthSmile - base.mouthSmile > 0.08)  cues.push('Corners â†â†’');
      if (feat.mouthFrown - base.mouthFrown > 0.010) cues.push('Corners â†“');
      if (base.lipGap - feat.lipGap > 0.010)         cues.push('Lip press');
      if (base.outerEyeGap - feat.outerEyeGap > 0.006) cues.push('Eye crinkle');
      return cues;
    }

    async function init(){
      try { await faceapi.tf.setBackend('webgl'); } catch(_) {}
      await loadModels();
      await startCamera();
      const canvas = prepareCanvas();
      const ctx = canvas.getContext('2d');

      const detOpts = () => new faceapi.TinyFaceDetectorOptions({ inputSize: highPerf ? 320 : 224, scoreThreshold: 0.4 });
      const detect = () => faceapi
        .detectSingleFace(video, detOpts())
        .withFaceLandmarks()
        .withFaceExpressions();

      // warm up + calibration
      await detect();
      await calibrate(detect, 2000);

      let frames=0, mark=performance.now();
      async function loop(){
        try{
          const det = await detect();
          ctx.clearRect(0,0,canvas.width,canvas.height);

          if (det){
            const resized = faceapi.resizeResults(det, { width: canvas.width, height: canvas.height });
            faceapi.draw.drawDetections(canvas, resized);
            faceapi.draw.drawFaceLandmarks(canvas, resized);
            faceapi.draw.drawFaceExpressions(canvas, resized);

            const exps = det.expressions;
            const feat = calcFeatures(det.landmarks);

            // fused label
            const label = labelEmotion(exps, feat);
            chipMain.textContent = `${EMOJI[label]||'ðŸ™‚'} ${label[0].toUpperCase()+label.slice(1)}`;

            // cues explainer
            const cues = subtleCueText(feat);
            if (cues.length){ chipSubtle.style.display='block'; chipSubtle.textContent = 'Subtle: ' + cues.join(', '); }
            else { chipSubtle.style.display='none'; }

            statusEl.textContent = 'Trackingâ€¦';
          } else {
            statusEl.textContent = 'No face detected';
            chipSubtle.style.display='none';
          }

          // FPS meter
          frames++; const now=performance.now();
          if (now - mark > 1000){ fpsEl.textContent = frames + ' fps'; frames=0; mark=now; }
        }catch(err){ showErr('Detection error: ' + err.message); }
        requestAnimationFrame(loop);
      }
      requestAnimationFrame(loop);

      btnRecal.onclick = async () => { calibrated=false; await calibrate(detect, 2000); };
      btnPerf.onclick  = () => { highPerf = !highPerf; btnPerf.textContent='Performance: ' + (highPerf?'High':'Normal'); location.reload(); };
    }

    init().catch(e=>showErr('Init error: ' + e.message));
  </script>
</body>
</html>
