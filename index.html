<script>
(async () => {
  const cam = document.getElementById('cam');
  const overlay = document.getElementById('overlay');
  const ctx = overlay.getContext('2d', { willReadFrequently:true });
  const diag = (...a)=>{ const el = document.getElementById('diag'); console.log('[diag]',...a); el.textContent += a.join(' ') + '\n'; el.scrollTop = el.scrollHeight; };

  // 1) Plain camera, no deviceId/facingMode — plus a 7s timeout
  async function getPlainCamera() {
    diag('getUserMedia attempt (plain): {"video":true,"audio":false}');
    let timer;
    const timeout = new Promise((_,rej)=>{ timer=setTimeout(()=>rej(new Error('getUserMedia timeout (plain)')), 7000); });
    try {
      const stream = await Promise.race([ navigator.mediaDevices.getUserMedia({ video:true, audio:false }), timeout ]);
      clearTimeout(timer);
      return stream;
    } catch (e) {
      clearTimeout(timer);
      throw e;
    }
  }

  let stream;
  try {
    stream = await getPlainCamera();
  } catch (e) {
    diag('FATAL: plain getUserMedia failed:', e.name || 'Error', '-', e.message || e);
    return;
  }

  // 2) Attach + size
  cam.srcObject = stream;
  try { await cam.play(); } catch (e) { diag('video.play error:', e.message); }
  await new Promise(r => cam.onloadedmetadata = r);
  overlay.width  = cam.videoWidth  || 640;
  overlay.height = cam.videoHeight || 480;
  diag('Webcam OK @', cam.videoWidth, 'x', cam.videoHeight);

  // 3) Load models from your GitHub (Pages → Raw fallback)
  async function probe(base){
    const u=`${base}/tiny_face_detector_model-weights_manifest.json?cb=${Date.now()}`;
    const r=await fetch(u,{cache:'no-store'});
    diag('PROBE', u, '→', r.status);
    return r.ok ? base : null;
  }
  const pages = 'https://geraldsathiyasiva.github.io/Emotion-Ai-/models';
  const raw   = 'https://raw.githubusercontent.com/GeraldSathiyasiva/Emotion-Ai-/main/models';
  const MODEL_URL = (await probe(pages)) || (await probe(raw));
  if (!MODEL_URL) { diag('FATAL: models not reachable'); return; }

  try {
    if (faceapi?.tf?.setBackend) { await faceapi.tf.setBackend('webgl'); await faceapi.tf.ready(); }
    diag('TFJS backend:', faceapi.tf?.getBackend ? faceapi.tf.getBackend() : 'n/a');
    await faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL);
    await faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL);
    await faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL);
    diag('MODELS: OK');
  } catch (e) {
    diag('FATAL: model load:', e.message || e); return;
  }

  // 4) 5s interval test (guaranteed per-tick logs)
  function draw() {
    const mirror = document.getElementById('mirror')?.checked;
    const w = overlay.width, h = overlay.height;
    ctx.save(); ctx.clearRect(0,0,w,h);
    if (mirror) { ctx.translate(w,0); ctx.scale(-1,1); }
    ctx.drawImage(cam,0,0,w,h); ctx.restore();
  }
  function luminance(){
    const w=overlay.width,h=overlay.height;
    const x=Math.max(0,Math.floor(w/2)-32), y=Math.max(0,Math.floor(h/2)-32);
    const d=ctx.getImageData(x,y,Math.min(64,w-x),Math.min(64,h-y)).data;
    let s=0,n=0; for(let i=0;i<d.length;i+=4){ s+=0.2126*d[i]+0.7152*d[i+1]+0.0722*d[i+2]; n++; }
    return n? s/n : 0;
  }
  async function detectOnce(videoEl){
    const tries=[
      {inputSize:640,scoreThreshold:0.025},
      {inputSize:512,scoreThreshold:0.025},
      {inputSize:384,scoreThreshold:0.030},
      {inputSize:320,scoreThreshold:0.040},
    ];
    for (const t of tries){
      const res = await faceapi.detectAllFaces(videoEl,new faceapi.TinyFaceDetectorOptions(t)).withFaceLandmarks().withFaceExpressions();
      if (res && res.length) return res[0];
    }
    return null;
  }

  diag('Test starting…');
  const SAMPLE_MS = 250, TEST_MS = 5000;
  let ticks=0, detects=0;
  const t0 = performance.now();
  const iv = setInterval(async ()=>{
    if (performance.now()-t0 >= TEST_MS) {
      clearInterval(iv);
      diag(`Test done — detections=${detects}/${ticks}`);
      // enable Play if desired:
      const btnPlay = document.getElementById('btnPlay');
      if (btnPlay) btnPlay.disabled = detects===0;
      return;
    }
    ticks++;
    draw();
    const Y = luminance();
    let det=null; try { det = await detectOnce(cam); } catch(e){ diag('detectOnce error:', e.message); }
    if (det) {
      detects++;
      faceapi.draw.drawDetections(overlay, [det]);
      faceapi.draw.drawFaceLandmarks(overlay, [det]);
      const e = det.expressions || {};
      diag(`✓ score=${det.detection.score.toFixed(3)} lum=${Y.toFixed(1)} happy=${(e.happy||0).toFixed(2)} neutral=${(e.neutral||0).toFixed(2)}`);
    } else {
      diag(`× no face (lum=${Y.toFixed(1)})`);
    }
  }, SAMPLE_MS);
})();
</script>
