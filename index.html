<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<title>Emotion Autorun — Camera + GitHub Models + 5s Test</title>
<meta name="viewport" content="width=device-width,initial-scale=1" />
<style>
  :root { color-scheme: dark; }
  body{background:#0f1115;color:#e5e7eb;font-family:system-ui,Arial;margin:16px}
  video,canvas{border:1px solid #2a2f3d;border-radius:10px;background:#000;max-width:100%}
  #diag{white-space:pre-wrap;font:12px ui-monospace;background:#0b1020;border:1px solid #222;border-radius:10px;padding:10px;height:320px;overflow:auto;margin-top:10px}
  .pill{display:inline-block;padding:2px 8px;background:#1f2433;border-radius:8px;margin-left:8px}
</style>

<script>
/* Early logger so we see **anything** that happens */
(function(){
  const Q=[]; window.__diagReady=false; window.__diagQ=Q;
  window.diag=function(){ const s=[...arguments].join(' '); console.log('[diag]',s);
    if(!window.__diagReady){Q.push(s);return;}
    const el=document.getElementById('diag'); if(el){ el.textContent+=s+'\n'; el.scrollTop=el.scrollHeight; }
  };
  window.addEventListener('error', e=>diag(`JS ERROR: ${e.message} @ ${e.filename}:${e.lineno}`));
  window.addEventListener('unhandledrejection', e=>diag(`Promise REJECTION: ${e.reason?.message||e.reason}`));
  diag('BOOT: inline logger alive');
})();
</script>

<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.18.0/dist/tf.min.js?v=autorun"
        onload="diag('CDN: tfjs loaded')" onerror="diag('CDN: tfjs **FAILED** to load')"></script>
<script src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js?v=autorun"
        onload="diag('CDN: face-api loaded')" onerror="diag('CDN: face-api **FAILED** to load')"></script>
</head>
<body>
<h2>Autorun Face/Emotion 5s Test
  <span class="pill">frames: <span id="pf">0</span></span>
  <span class="pill">faces: <span id="pc">0</span></span>
  <span class="pill">build: autorun-001</span>
</h2>

<video id="cam" width="640" height="480" autoplay muted playsinline></video>
<canvas id="ov" width="640" height="480"></canvas>
<div id="diag">DIAG:\n</div>

<script>
/* Make DIAG live */
window.__diagReady=true; if(Array.isArray(window.__diagQ)){ window.__diagQ.forEach(s=>diag(s)); window.__diagQ.length=0; }

(async ()=>{
  const MODEL_PAGES='https://geraldsathiyasiva.github.io/Emotion-Ai-/models';
  const MODEL_RAW  ='https://raw.githubusercontent.com/GeraldSathiyasiva/Emotion-Ai-/main/models';
  const SAMPLE_MS=250, TEST_MS=5000;

  const cam=document.getElementById('cam');
  const ov=document.getElementById('ov');
  const ctx=ov.getContext('2d',{willReadFrequently:true});
  const pf=document.getElementById('pf'), pc=document.getElementById('pc');

  diag('ENV secure/localhost:', (location.protocol==='https:' || location.hostname==='localhost'));
  diag('UA:', navigator.userAgent);

  /* 1) Start webcam immediately */
  try{
    const stream = await navigator.mediaDevices.getUserMedia({
      video:{ facingMode:'user', width:{ideal:1280}, height:{ideal:720} }, audio:false
    });
    cam.srcObject=stream; await cam.play(); await new Promise(r=>cam.onloadedmetadata=r);
    ov.width=cam.videoWidth||640; ov.height=cam.videoHeight||480;
    diag('Webcam OK @', cam.videoWidth, 'x', cam.videoHeight);
  }catch(e){
    diag('FATAL: getUserMedia:', e.name, e.message);
    return;
  }

  /* 2) Pick model host (Pages → Raw) */
  async function probe(base){
    const u=`${base}/tiny_face_detector_model-weights_manifest.json?cb=${Date.now()}`;
    try{ const r=await fetch(u,{cache:'no-store'}); diag('PROBE', u, '→', r.status); return r.ok? base : null; }
    catch(err){ diag('PROBE ERR', err.message); return null; }
  }
  let MODEL_URL = await probe(MODEL_PAGES) || await probe(MODEL_RAW);
  if(!MODEL_URL){ diag('FATAL: models not reachable'); return; }
  diag('MODEL_URL =', MODEL_URL);

  /* 3) Load models */
  try{
    if(faceapi?.tf?.setBackend){ try{ await faceapi.tf.setBackend('webgl'); await faceapi.tf.ready(); }catch{} }
    diag('TFJS backend:', (faceapi.tf?.getBackend && faceapi.tf.getBackend()) || 'n/a');
    await faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL);
    await faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL);
    await faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL);
    diag('MODELS: OK');
  }catch(e){
    diag('FATAL: model load:', e.message); return;
  }

  /* 4) Detection helpers */
  function drawFrame(){
    ctx.clearRect(0,0,ov.width,ov.height);
    ctx.drawImage(cam,0,0,ov.width,ov.height);
  }
  function luminance(){
    const x=Math.max(0,Math.floor(ov.width/2)-32), y=Math.max(0,Math.floor(ov.height/2)-32);
    const w=Math.min(64,ov.width-x), h=Math.min(64,ov.height-y);
    if(w<=0||h<=0) return 0;
    const d=ctx.getImageData(x,y,w,h).data; let s=0,n=0;
    for(let i=0;i<d.length;i+=4){ s+=0.2126*d[i]+0.7152*d[i+1]+0.0722*d[i+2]; n++; }
    return n? s/n : 0;
  }
  async function detectOnce(videoEl){
    const tries=[
      {inputSize:640,scoreThreshold:0.025},
      {inputSize:512,scoreThreshold:0.025},
      {inputSize:384,scoreThreshold:0.030},
      {inputSize:320,scoreThreshold:0.040},
    ];
    for(const t of tries){
      const res=await faceapi.detectAllFaces(videoEl,new faceapi.TinyFaceDetectorOptions(t)).withFaceLandmarks().withFaceExpressions();
      if(res && res.length) return res[0];
    }
    return null;
  }

  /* 5) Autorun 5s test with setInterval (guaranteed ticks) */
  diag('Test starting…');
  let ticks=0, detects=0;
  const t0=performance.now();
  const iv=setInterval(async ()=>{
    try{
      if(performance.now()-t0>=TEST_MS){
        clearInterval(iv);
        diag(`Test done — detections=${detects}/${ticks} | manifest: ${MODEL_URL}/tiny_face_detector_model-weights_manifest.json`);
        return;
      }
      ticks++; pf.textContent=String(ticks);
      drawFrame();
      const Y=luminance();
      let det=null;
      try{ det=await detectOnce(cam); }catch(e){ diag('detectOnce error:', e.message); }
      if(det){
        detects++; pc.textContent=String(detects);
        // draw expects arrays:
        faceapi.draw.drawDetections(ov, [det]);
        faceapi.draw.drawFaceLandmarks(ov, [det]);
        const e=det.expressions||{};
        diag(`✓ score=${det.detection.score.toFixed(3)} lum=${Y.toFixed(1)} happy=${(e.happy||0).toFixed(2)} neutral=${(e.neutral||0).toFixed(2)}`);
      }else{
        diag(`× no face (lum=${Y.toFixed(1)})`);
      }
    }catch(err){
      diag('Tick error:', err?.message||err);
    }
  }, SAMPLE_MS);
})();
</script>
</body>
</html>
