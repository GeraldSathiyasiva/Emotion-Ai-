<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<title>Emotion Ad Test — Report (PI / Brand / Emotions) · v5.1</title>
<style>
  :root { color-scheme: dark; }
  body { font-family: system-ui, -apple-system, Segoe UI, Roboto, Arial; margin:16px; background:#0f1115; color:#fff; }
  .row { display:flex; flex-wrap:wrap; gap:14px; align-items:flex-start; }
  .panel { flex:1 1 360px; border:1px solid #222; border-radius:12px; padding:12px; background:#131722; }
  label { display:block; font-weight:600; margin-top:10px; }
  input[type="text"] { width:100%; padding:8px 10px; border-radius:8px; border:1px solid #333; background:#161a26; color:#fff; }
  .btns { display:flex; gap:10px; align-items:center; margin-top:12px; flex-wrap:wrap; }
  button { padding:10px 14px; border:0; border-radius:10px; background:#2a2f3d; color:#fff; cursor:pointer; }
  button:disabled { opacity:.55; cursor:not-allowed; }
  video, canvas { width:100%; border-radius:12px; background:#000; }
  .kpi { display:flex; gap:10px; margin-top:10px; }
  .kpi>div { flex:1 1 0; background:#101520; border:1px solid #222; border-radius:10px; padding:10px; text-align:center; }
  .muted { color:#9aa3b2; font-size:12px; }
  .badge { display:inline-block; padding:2px 8px; border-radius:8px; font-size:12px; background:#7c2d12; margin-left:8px; }
  .ok { background:#14532d; }
  #diag, #reportBox { font:12px/1.35 ui-monospace, SFMono-Regular, Menlo, Consolas, monospace; white-space:pre-wrap; color:#cbd5e1; margin-top:8px; max-height:260px; overflow:auto; border:1px solid #222; border-radius:8px; padding:8px; background:#0d1220; }
  .pill { display:inline-block; padding:2px 6px; border-radius:6px; background:#1f2433; margin-left:6px; font:12px ui-monospace; }
</style>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.18.0/dist/tf.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>
</head>
<body>

<h2>Emotion-Only Ad Test (Webcam → Report)
  <span id="samplestate" class="badge">IDLE</span>
  <span class="pill">Tick: <span id="tick">0</span></span>
  <span class="pill">pushes: <span id="pPushes">0</span></span>
  <span class="pill">samples: <span id="pSamples">0</span></span>
  <span class="pill">fps: <span id="fps">0</span></span>
</h2>

<div class="row">
  <div class="panel" style="flex:1 1 560px;">
    <label>Ad URL (MP4 — HTTPS or repo path, e.g. <code>ads/0813.mp4</code>)</label>
    <input id="adUrl" type="text" placeholder="ads/0813.mp4" />

    <div class="panel" style="margin-top:10px;">
      <b>Brand moment windows (seconds: start,end)</b>
      <label>Logo</label> <input id="logoWin" type="text" value="2,4" />
      <label>Packshot</label> <input id="packWin" type="text" value="8,10" />
      <label>CTA</label> <input id="ctaWin" type="text" value="14,17" />
      <div class="muted" style="margin-top:6px;">Use times that clearly fall inside your ad.</div>
    </div>

    <div class="btns">
      <button id="btnPlay" disabled>Play & analyze</button>
      <button id="btnCSV" disabled>Download CSV (frames)</button>
      <button id="btnJSON" disabled>Download JSON (report)</button>
      <span id="status" class="muted">Booting…</span>
    </div>

    <div class="row" style="margin-top:10px;">
      <div style="flex:1 1 380px;">
        <video id="ad" controls playsinline></video>
      </div>
      <div style="flex:1 1 220px; position:relative;">
        <video id="cam" autoplay muted playsinline style="width:100%;height:auto;"></video>
        <canvas id="overlay" style="width:100%;height:auto;"></canvas>
      </div>
    </div>

    <div class="kpi">
      <div><div class="muted">Positive Recognition</div><div id="kPR">—</div></div>
      <div><div class="muted">Awareness</div><div id="kAWR">—</div></div>
      <div><div class="muted">Purchase Intention</div><div id="kPI">—</div></div>
    </div>

    <div id="reportBox">Report will appear here after analysis…</div>
    <div id="diag">DIAG:\n</div>
  </div>

  <div class="panel" style="flex:1 1 320px;">
    <b>Notes</b>
    <ul class="muted">
      <li>Models load from <code>/Emotion-Ai-/models</code>. DIAG will show “MODELS: OK”.</li>
      <li>Sampler starts immediately; samples increase even before models load.</li>
      <li>We log live expression values; if they’re ~0, improve lighting and look frontal.</li>
    </ul>
  </div>
</div>

<script>
  // ======= CONFIG =======
  const MODEL_URL = '/Emotion-Ai-/models'; // matches your GitHub Pages repo
  const SAMPLE_MS = 150, BASELINE_PRE=[2.0,0.5], RESP_DUR=2.0;

  // ======= DOM =======
  const $ = id => document.getElementById(id);
  const ad = $('ad'), cam = $('cam'), canv = $('overlay'), ctx = canv.getContext('2d');
  const statusEl=$('status'), fpsEl=$('fps'), badge=$('samplestate'), diagEl=$('diag'), reportEl=$('reportBox');
  const pSamples=$('pSamples'), pPushes=$('pPushes'), tickEl=$('tick');
  const kPR=$('kPR'), kAWR=$('kAWR'), kPI=$('kPI');

  // ======= STATE =======
  let modelsReady=false, webcamReady=false, samplingTimer=null, startedAtMs=null;
  let framesCount=0, fpsT0=performance.now(), pushes=0, tick=0;
  const stream=[]; let lastNose=null;
  let faceFrames = 0; // <— B) face frame counter

  // ======= UTILS =======
  const log=(...a)=>{ console.log(...a); statusEl.textContent=a.join(' '); };
  const diag=(...a)=>{ console.log('[diag]',...a); diagEl.textContent += a.join(' ') + '\n'; };
  const clamp=(x,a,b)=>Math.min(b,Math.max(a,x));
  const mean=a=>a.length?a.reduce((x,y)=>x+y,0)/a.length:0;
  const peak=a=>a.length?Math.max(...a):0;
  const stdev=a=>{ if(!a.length) return 0; const m=mean(a); return Math.sqrt(a.reduce((s,x)=>s+(x-m)*(x-m),0)/a.length); };
  const sigmoid=x=>1/(1+Math.exp(-x));
  const parseWin = s => { const [a,b]=(s||'').split(',').map(Number); return [Number(a||0),Number(b||0)]; };
  const inWindow = (t, w) => t>=w[0] && t<=w[1];
  function setBadge(active){ badge.textContent = active ? 'SAMPLING' : 'IDLE'; badge.className='badge '+(active?'ok':''); }

  // ======= FEATURES =======
  function eyeAspectRatio(p, ids){
    const v = (p[ids[1]].y - p[ids[5]].y + p[ids[2]].y - p[ids[4]].y)/2;
    const h = (p[ids[3]].x - p[ids[0]].x);
    return Math.max(0, v/(Math.abs(h)+1e-5));
  }
  function featuresFromLandmarks(det){
    const p = det.landmarks.positions;
    const earL = eyeAspectRatio(p,[36,37,38,39,40,41]);
    const earR = eyeAspectRatio(p,[42,43,44,45,46,47]);
    const eyesOpen = clamp((earL+earR)/2 * 12, 0, 1);

    const nose = p[30], left=p[2], right=p[14], top=p[27], bottom=p[8];
    const cx=(left.x+right.x)/2, cy=(top.y+bottom.y)/2;
    const yaw  = clamp((nose.x-cx)/(Math.abs(right.x-left.x)+1e-5), -0.6, 0.6);
    const pitch= clamp((nose.y-cy)/(Math.abs(bottom.y-top.y)+1e-5), -0.6, 0.6);

    const gazeX = clamp(0.5 + yaw * -0.9, 0, 1);
    const gazeY = clamp(0.5 + pitch *  0.9, 0, 1);

    let jitter=0; if (lastNose) jitter=Math.hypot(nose.x-lastNose.x, nose.y-lastNose.y);
    lastNose = {x:nose.x,y:nose.y};
    return { eyesOpen, gazeX, gazeY, jitter };
  }

  function valenceFromExpr(e){
    const pos = (e.happy||0) + 0.6*(e.surprised||0);
    const neg = (e.angry||0) + (e.sad||0) + (e.disgusted||0) + 0.5*(e.fearful||0);
    return clamp(pos - neg, -1, 1);
  }
  function arousalFromExpr(e){
    return clamp((e.surprised||0)+(e.fearful||0)+(e.angry||0)+0.5*(e.happy||0), 0, 1);
  }
  function amusedFromExpr(e){ return clamp(0.5*((e.happy||0)+(e.surprised||0)), 0, 1); }

  // ======= WINDOWS / DELTAS =======
  function windowSlice(t0,t1){ return stream.filter(r=>r.t>=t0 && r.t<t1); }
  function windowStats(t0,t1,key){
    const rows = windowSlice(t0,t1);
    const arr = rows.map(r=>r[key]);
    return { mean: mean(arr), peak: peak(arr), std: stdev(arr) };
  }
  function deltaFeature(te, key){
    const base = windowStats(te-BASELINE_PRE[0], te-BASELINE_PRE[1], key);
    const resp = windowStats(te, te+RESP_DUR, key);
    return { dMean: resp.mean - base.mean, dPeak: resp.peak - base.peak, base, resp };
  }

  // ======= KPIs =======
  function engagementIndexSimple(t0, dur){
    const eyes = windowStats(t0, t0+dur, 'att').mean;
    const jit = stdev(windowSlice(t0, t0+dur).map(r=>r.jitter));
    const jitterN = Math.max(0, 1 - (jit/2.0));
    return 0.7*eyes + 0.3*jitterN;
  }
  function computeKPIs(winLogo, winPack, winCTA){
    const tLogo=winLogo[0], tPack=winPack[0], tCTA=winCTA[0];
    const dV_logo = deltaFeature(tLogo, 'val');
    const dV_pack = deltaFeature(tPack, 'val');
    const baseAll = windowStats(0, Math.max(0.1, tLogo), 'val');
    const z = (x, m=baseAll.mean, s=0.25)=> s>1e-6 ? (x-m)/s : 0;
    const PR = 100/(1+Math.exp(-(0.95*z(dV_logo.dMean)+0.55*z(dV_pack.dMean))));
    const dA_logo = deltaFeature(tLogo, 'aro').dMean;
    const AWR = 100/(1+Math.exp(-(0.9*z(dA_logo))));
    const wCTA = windowStats(tCTA, tCTA+Math.max(2.5, RESP_DUR), 'val');
    const v_cta=wCTA.mean;
    const eng_cta = engagementIndexSimple(tCTA, Math.max(2.5, RESP_DUR));
    const PIraw = 100/(1+Math.exp(-(0.85*z(v_cta,0,0.3)+0.55*z(eng_cta,0.3,0.25))));
    return { PR:+PR.toFixed(1), AWR:+AWR.toFixed(1), PI:+PIraw.toFixed(1) };
  }

  // ======= Positive/Negative splits =======
  function pct(x){ return Math.max(0, Math.min(100, +x.toFixed(1))); }
  function posNegFromScore(score){ const pos = clamp(score,0,100); return { Positive:pct(pos), Negative:pct(100-pos) }; }
  function brandPerception(winLogo, winPack){
    const tLogo=winLogo[0];
    const base = windowStats(Math.max(0,tLogo-3), Math.max(0,tLogo-0.5), 'val').mean;
    const brandVal = mean([
      windowStats(winLogo[0], winLogo[1], 'val').mean,
      windowStats(winPack[0], winPack[1], 'val').mean
    ]);
    const s = 100/(1+Math.exp(-( (brandVal - base) / 0.2 ))); // 0–100
    return posNegFromScore(s);
  }
  function purchaseIntention(winCTA){
    const s = computeKPIs([0,0],[0,0],winCTA).PI;
    return posNegFromScore(s);
  }

  // ======= Emotion peaks =======
  function extractPeaks(emokey, threshold=0.5, minSep=0.6){
    const out = []; let last = -999;
    for (const r of stream){
      if (r.face && r[emokey] !== undefined && r[emokey] >= threshold){
        if (r.t - last >= minSep){ out.push(+r.t.toFixed(2)); last = r.t; }
      }
    }
    return out;
  }

  // ======= EXPORT =======
  function toCSV(rows){
    const header = ['t','att','val','aro','happy','sad','angry','disgusted','fearful','surprised','neutral','amused','gazeX','gazeY','jitter','face'];
    const lines = [header.join(',')];
    rows.forEach(r => lines.push([
      r.t,r.att,r.val,r.aro,r.happy,r.sad,r.angry,r.disgusted,r.fearful,r.surprised,r.neutral,r.amused,r.gazeX,r.gazeY,r.jitter,r.face
    ].map(x=>typeof x==='number'?x.toFixed(4):x).join(',')));
    return lines.join('\n');
  }
  function download(name, text, type='text/plain'){
    const blob = new Blob([text], {type}); const a=document.createElement('a');
    a.href=URL.createObjectURL(blob); a.download=name; a.click(); URL.revokeObjectURL(a.href);
  }

  // ======= MODELS =======
  async function loadModels(){
    try{
      const probe = async f => {
        const full = `${MODEL_URL}/${f}`;
        diag('Probing', full);
        const r = await fetch(full, { cache:'no-store' });
        if(!r.ok) throw new Error(`PROBE ${r.status}: ${full}`);
      };
      await probe('tiny_face_detector_model-weights_manifest.json');
      await probe('face_landmark_68_model-weights_manifest.json');
      await probe('face_expression_model-weights_manifest.json');

      log('Loading models…');
      await faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL);
      await faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL);
      await faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL);
      modelsReady = true;
      diag('MODELS: OK ✔ tinyFace + landmarks68 + expressions');
    }catch(e){ diag('Model load ERROR:', e.message); }
  }

  // ======= WEBCAM =======
  async function startWebcam(){
    diag('Requesting webcam…');
    const st = await navigator.mediaDevices.getUserMedia({ video:{facingMode:'user', width:{ideal:640}, height:{ideal:480}}, audio:false });
    cam.srcObject = st; await cam.play();
    canv.width = cam.videoWidth || 640; canv.height = cam.videoHeight || 480;
    webcamReady = true; log(`Webcam ready ${canv.width}x${canv.height}.`);
    $('btnPlay').disabled = false;
  }

  // ======= SAMPLING =======
  async function sampleOnce(){
    try{
      tick++; tickEl.textContent=String(tick);

      const tNow = (isFinite(ad.currentTime) && ad.currentTime>0)
        ? +ad.currentTime.toFixed(3)
        : (startedAtMs ? ((performance.now()-startedAtMs)/1000).toFixed(3) : 0);

      ctx.clearRect(0,0,canv.width,canv.height);
      ctx.drawImage(cam, 0, 0, canv.width, canv.height);

      let row = { t:+tNow, att:0, val:0, aro:0,
        happy:0, sad:0, angry:0, disgusted:0, fearful:0, surprised:0, neutral:0, amused:0,
        gazeX:0.5, gazeY:0.5, jitter:0, face:0 };

      if (modelsReady){
        // A) aggressive detection tries
        const tries = [
          { inputSize: 320, scoreThreshold: 0.12 },
          { inputSize: 256, scoreThreshold: 0.12 },
          { inputSize: 224, scoreThreshold: 0.10 },
          { inputSize: 192, scoreThreshold: 0.08 },
          { inputSize: 160, scoreThreshold: 0.06 }
        ];
        let det = null;
        for (const opt of tries){
          const res = await faceapi
            .detectAllFaces(cam, new faceapi.TinyFaceDetectorOptions(opt))
            .withFaceLandmarks()
            .withFaceExpressions();
          if (res && res.length){
            det = res.reduce((best,cur)=> (cur.detection.score > (best?.detection.score||0) ? cur : best), null);
            break;
          }
        }

        if (det){
          const resized = faceapi.resizeResults(det, { width: canv.width, height: canv.height });
          faceapi.draw.drawDetections(canv, resized);
          faceapi.draw.drawFaceLandmarks(canv, resized);

          const feats = featuresFromLandmarks(det);
          const e = det.expressions || {};

          row.happy = +(e.happy||0);
          row.sad = +(e.sad||0);
          row.angry = +(e.angry||0);
          row.disgusted = +(e.disgusted||0);
          row.fearful = +(e.fearful||0);
          row.surprised = +(e.surprised||0);
          row.neutral = +(e.neutral||0);
          row.amused = amusedFromExpr(e);

          row.val = valenceFromExpr(e);
          row.aro = arousalFromExpr(e);
          row.att = clamp(((feats.eyesOpen)||0),0,1);
          row.gazeX = feats.gazeX; row.gazeY = feats.gazeY; row.jitter = feats.jitter; row.face=1;

          // B) count face frames + D) live expression readout
          faceFrames++;
          if (faceFrames % 20 === 0) diag(`faces so far: ${faceFrames}/${stream.length}`);
          diag(`expr t=${row.t.toFixed(2)} happy=${row.happy.toFixed(2)} sad=${row.sad.toFixed(2)} ang=${row.angry.toFixed(2)} dis=${row.disgusted.toFixed(2)} surp=${row.surprised.toFixed(2)} fear=${row.fearful.toFixed(2)}`);
        } else {
          diag('No face this frame.');
        }
      }

      stream.push(row);
      pushes++; pPushes.textContent=String(pushes);
      pSamples.textContent=String(stream.length);

      framesCount++; const now = performance.now();
      if (now - fpsT0 > 1000){ fpsEl.textContent = String(framesCount); framesCount=0; fpsT0 = now; }

      setBadge(true); statusEl.textContent = `Recording… samples: ${stream.length}`;
    }catch(e){ diag('sampleOnce error:', e.message); }
  }
  function startSampling(){
    if (!webcamReady) return;
    stream.length=0; lastNose=null; pushes=0; tick=0; startedAtMs=performance.now();
    faceFrames = 0;
    if (samplingTimer) clearInterval(samplingTimer);
    samplingTimer = setInterval(sampleOnce, SAMPLE_MS);
    diag('Sampling started');
  }
  function stopSampling(){ if (samplingTimer){ clearInterval(samplingTimer); samplingTimer=null; setBadge(false); diag('Sampling stopped'); } }

  // ======= PLAY & ANALYZE =======
  $('btnPlay').addEventListener('click', async () => {
    const url = $('adUrl').value.trim();
    if (!url) return log('Enter a valid Ad URL (e.g., ads/0813.mp4)');
    ad.src = url;
    try { await ad.play(); } catch(e) { log('Click the video to start playback (autoplay blocked)'); }
    ad.onpause = () => log('Paused');
    ad.onended = () => { stopSampling(); log('Analyzing…'); analyzeAndShow(); };
  });

  function computeKPIsPublic(winLogo, winPack, winCTA){
    const KPIs = computeKPIs(winLogo, winPack, winCTA);
    kPR.textContent = KPIs.PR.toString();
    kAWR.textContent = KPIs.AWR.toString();
    kPI.textContent = KPIs.PI.toString();
    return KPIs;
  }

  function buildReport(){
    const winLogo = parseWin($('logoWin').value);
    const winPack = parseWin($('packWin').value);
    const winCTA  = parseWin($('ctaWin').value);

    const KPIs = computeKPIsPublic(winLogo, winPack, winCTA);
    const PI = purchaseIntention(winCTA);
    const BP = brandPerception(winLogo, winPack);

    // C) lower thresholds for peaks
    const peaks = {
      Happy:      extractPeaks('happy',     0.35, 0.6),
      Amused:     extractPeaks('amused',    0.30, 0.6),
      Sad:        extractPeaks('sad',       0.35, 0.6),
      Angry:      extractPeaks('angry',     0.35, 0.6),
      Surprised:  extractPeaks('surprised', 0.35, 0.6)
    };

    const suggestions = {};
    if (!peaks.Happy.length) suggestions.Happy = "Consider adding a smile/humor beat near CTA.";
    if (!peaks.Amused.length) suggestions.Amused = "Add a light surprise + smile moment before packshot.";
    if (!peaks.Surprised.length) suggestions.Surprised = "Introduce a visual reveal around the logo.";

    return {
      meta: { ts: Date.now(), ad: $('adUrl').value },
      kpi_raw: KPIs,
      PurchaseIntention: PI,
      BrandPerception: BP,
      EmotionsToAdd: peaks,
      suggestions
    };
  }

  function analyzeAndShow(){
    if (!stream.length){ log('No samples captured.'); return; }
    const report = buildReport();
    reportEl.textContent = JSON.stringify(report, null, 2);
    $('btnCSV').disabled = false; $('btnJSON').disabled = false;
    log('Report ready. You can download JSON/CSV.');
  }

  // ======= DOWNLOADS =======
  $('btnCSV').addEventListener('click', () => download(`frames_${Date.now()}.csv`, toCSV(stream), 'text/csv'));
  $('btnJSON').addEventListener('click', () => {
    const report = buildReport();
    download(`report_${Date.now()}.json`, JSON.stringify(report, null, 2), 'application/json');
  });

  // ======= BOOT =======
  window.addEventListener('error', e=>diag('window.onerror:', e.message));
  (async () => {
    log('Booting…');
    diag('Page loaded. Starting webcam & sampling; loading models in background.');
    try { await startWebcam(); } catch(e){ diag('webcam error:', e.message); alert('Camera error: '+e.message); }
    startSampling();
    loadModels();
  })();
</script>
</body>
</html>
