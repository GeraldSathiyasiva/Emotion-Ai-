<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Emotion + Subtle Cues (no-download)</title>

  <!-- face-api.js -->
  <script src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>

  <style>
    :root { color-scheme: dark; }
    body { margin:0; min-height:100vh; display:flex; flex-direction:column; align-items:center; justify-content:center; background:#111; color:#fff; font-family:system-ui,-apple-system,Arial; }
    #container { position:relative; width:100%; max-width:420px; }
    video { width:100%; border-radius:12px; background:#000; display:block; }
    canvas { position:absolute; top:0; left:0; pointer-events:none; }
    #hud { width:100%; max-width:420px; display:flex; align-items:center; justify-content:space-between; gap:10px; margin-top:10px; }
    #status { opacity:.95; }
    #fps { opacity:.7; }
    .chip {
      position:absolute; left:50%; transform:translateX(-50%);
      background:rgba(255,255,255,.12); padding:6px 12px; border-radius:999px;
      font-weight:600; backdrop-filter: blur(6px); box-shadow:0 6px 18px rgba(0,0,0,.2);
    }
    #chipMain { top:10px; }
    #chipSubtle { top:44px; display:none; background:rgba(88,170,255,.18); }
    .row { margin-top:10px; display:flex; gap:8px; align-items:center; }
    button { padding:8px 12px; border:0; border-radius:10px; background:#2a2a2a; color:#fff; }
  </style>
</head>
<body>
  <div id="container">
    <video id="video" autoplay muted playsinline></video>
    <!-- overlay -->
    <div id="chipMain"   class="chip">üôÇ Neutral</div>
    <div id="chipSubtle" class="chip">Subtle: ‚Äî</div>
  </div>

  <div id="hud">
    <div id="status">Loading models‚Ä¶</div>
    <div id="fps"></div>
  </div>

  <div class="row">
    <button id="recal">Re-calibrate (neutral)</button>
    <button id="perf">Performance: High</button>
  </div>

  <script>
    // Local models (you already have these 4 in /models): tinyFace + expressions
    const MODEL_URL = './models';
    // Hosted landmarks (no download needed)
    const MODEL_URL_LM = 'https://raw.githubusercontent.com/justadudewhohacks/face-api.js-models/master/face_landmark_68';

    const video = document.getElementById('video');
    const container = document.getElementById('container');
    const statusEl = document.getElementById('status');
    const fpsEl = document.getElementById('fps');
    const chipMain = document.getElementById('chipMain');
    const chipSubtle = document.getElementById('chipSubtle');
    const btnRecal = document.getElementById('recal');
    const btnPerf = document.getElementById('perf');

    let highPerf = true; // larger detector input + higher FPS hint
    const EMOJI = { neutral:'üòê', happy:'üòÉ', sad:'üò¢', angry:'üò†', fearful:'üò®', disgusted:'ü§¢', surprised:'üòÆ' };

    function showErr(m){ statusEl.textContent = m; statusEl.style.color = '#ff8a8a'; }

    async function loadModels(){
      statusEl.textContent = 'Loading models‚Ä¶';
      await faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL);
      await faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL);
      await faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL_LM); // <-- hosted; no download
      statusEl.textContent = 'Models loaded. Starting camera‚Ä¶';
    }

    async function startCamera(){
      const stream = await navigator.mediaDevices.getUserMedia({
        video: {
          facingMode:'user',
          width:{ ideal:1280 }, height:{ ideal:720 },
          frameRate: highPerf ? { ideal:60, max:120 } : { ideal:30, max:30 }
        },
        audio:false
      });
      video.srcObject = stream;
      await new Promise(res => { if (video.readyState >= 2) res(); else video.onloadedmetadata = () => res(); });
      try { await video.play(); } catch(_) {}
      statusEl.textContent = 'Camera started.';
    }

    function prepareCanvas(){
      const canvas = document.createElement('canvas');
      const sync=()=>{ const w=video.videoWidth||640, h=video.videoHeight||480; canvas.width=w; canvas.height=h; };
      sync(); container.appendChild(canvas);
      video.addEventListener('loadeddata', sync); video.addEventListener('resize', sync);
      return canvas;
    }

    // ---------- Subtle cues via landmarks vs baseline ----------
    const avgY = (pts, idxs) => idxs.reduce((s,i)=>s+pts[i].y,0)/idxs.length;
    const dist  = (a,b)=>Math.hypot(a.x-b.x, a.y-b.y);

    const LEFT_EYE   = [36,37,38,39,40,41];
    const RIGHT_EYE  = [42,43,44,45,46,47];
    const LEFT_BROW  = [17,18,19,20,21];
    const RIGHT_BROW = [22,23,24,25,26];
    const NOSE_L = 31, NOSE_R = 35;
    const M_L = 48, M_R = 54, M_TOP = 51, M_BOT = 57;

    const base = { browEyeGap:0, eyeAperture:0, mouthSmile:0, mouthFrown:0 };
    let calibrated = false;

    function calcFeatures(landmarks){
      const p = landmarks.positions;
      const faceH = dist(p[27], p[8]) || 1;

      const eyeApertureL = (p[38].y - p[40].y);
      const eyeApertureR = (p[44].y - p[46].y);
      const eyeAperture  = ((eyeApertureL + eyeApertureR)/2) / faceH;

      const browY_L = avgY(p, LEFT_BROW), browY_R = avgY(p, RIGHT_BROW);
      const eyeTopY_L = (p[37].y + p[38].y)/2, eyeTopY_R = (p[43].y + p[44].y)/2;
      const browEyeGap = (((eyeTopY_L - browY_L) + (eyeTopY_R - browY_R))/2) / faceH;

      const mouthW = dist(p[M_L], p[M_R]);
      const noseW  = dist(p[NOSE_L], p[NOSE_R]) || 1;
      const mouthSmile = mouthW / noseW;

      const mouthMidY = (p[M_TOP].y + p[M_BOT].y)/2;
      const cornerDrop = ((p[M_L].y - mouthMidY) + (p[M_R].y - mouthMidY))/2;
      const mouthFrown = cornerDrop / faceH;

      return { eyeAperture, browEyeGap, mouthSmile, mouthFrown };
    }

    async function calibrate(detector, ms=2000){
      statusEl.textContent = 'Calibrating‚Ä¶ relax face for 2s';
      const t0 = performance.now(); const samples = [];
      while (performance.now() - t0 < ms){
        const det = await detector();
        if (det?.landmarks){ samples.push(calcFeatures(det.landmarks)); }
        await new Promise(r=>requestAnimationFrame(r));
      }
      if (!samples.length) return false;
      const mean = k => samples.reduce((s,o)=>s+o[k],0)/samples.length;
      base.eyeAperture = mean('eyeAperture');
      base.browEyeGap  = mean('browEyeGap');
      base.mouthSmile  = mean('mouthSmile');
      base.mouthFrown  = mean('mouthFrown');
      calibrated = true;
      statusEl.textContent = 'Calibrated. Tracking‚Ä¶';
      return true;
    }

    function subtleLabels(feat){
      const labs = [];
      if (feat.browEyeGap - base.browEyeGap > 0.015) labs.push('Brow raise');
      if (feat.eyeAperture  - base.eyeAperture  > 0.012) labs.push('Eye widen');
      if (feat.mouthSmile   - base.mouthSmile   > 0.08)  labs.push('Corner pull');
      if (feat.mouthFrown   - base.mouthFrown   > 0.010) labs.push('Corner down');
      return labs;
    }

    async function init(){
      try { await faceapi.tf.setBackend('webgl'); } catch(_) {}
      await loadModels();
      await startCamera();
      const canvas = prepareCanvas();
      const ctx = canvas.getContext('2d');

      const detOpts = () => new faceapi.TinyFaceDetectorOptions({ inputSize: highPerf ? 320 : 224, scoreThreshold: 0.4 });
      const detect = () => faceapi
        .detectSingleFace(video, detOpts())
        .withFaceLandmarks()
        .withFaceExpressions();

      // warm up + calibration
      await detect();
      await calibrate(detect, 2000);

      let frames=0, mark=performance.now();
      async function loop(){
        try{
          const det = await detect();
          ctx.clearRect(0,0,canvas.width,canvas.height);

          if (det){
            const resized = faceapi.resizeResults(det, { width: canvas.width, height: canvas.height });
            faceapi.draw.drawDetections(canvas, resized);
            faceapi.draw.drawFaceLandmarks(canvas, resized);
            faceapi.draw.drawFaceExpressions(canvas, resized);

            const exps = det.expressions;
            const top = Object.keys(exps).reduce((a,b)=> exps[a] > exps[b] ? a : b);
            chipMain.textContent = `${EMOJI[top]||'üôÇ'} ${top[0].toUpperCase()+top.slice(1)}`;

            if (calibrated){
              const feat = calcFeatures(det.landmarks);
              const labs = subtleLabels(feat);
              if (labs.length){ chipSubtle.style.display='block'; chipSubtle.textContent = 'Subtle: ' + labs.join(', '); }
              else { chipSubtle.style.display='none'; }
            }
            statusEl.textContent = 'Tracking‚Ä¶';
          } else {
            statusEl.textContent = 'No face detected';
            chipSubtle.style.display='none';
          }

          frames++; const now=performance.now();
          if (now - mark > 1000){ fpsEl.textContent = frames + ' fps'; frames=0; mark=now; }
        }catch(err){ showErr('Detection error: ' + err.message); }
        requestAnimationFrame(loop);
      }
      requestAnimationFrame(loop);

      btnRecal.onclick = async () => { calibrated=false; await calibrate(detect, 2000); };
      btnPerf.onclick  = () => { highPerf = !highPerf; btnPerf.textContent='Performance: ' + (highPerf?'High':'Normal'); location.reload(); };
    }

    init().catch(e=>showErr('Init error: ' + e.message));
  </script>
</body>
</html>
